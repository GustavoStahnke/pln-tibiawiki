# üßô‚Äç‚ôÇÔ∏è Projeto de PLN ‚Äî TibiaWiki Itens

Reposit√≥rio do trabalho da disciplina de **Processamento de Linguagem Natural (PLN)** aplicando **vetoriza√ß√£o** (BoW, TF-IDF, SBERT), **similaridade**, **clusteriza√ß√£o (K-Means)**, **PCA** e **recomenda√ß√£o** para itens do Tibia, usando o **TibiaWiki** como fonte.

**Fonte de dados:** https://www.tibiawiki.com.br/wiki/Itens

---

## üéØ Objetivo

- Coletar e estruturar itens do Tibia (links ‚Üí p√°ginas ‚Üí atributos).  
- Pr√©-processar texto (normaliza√ß√£o, stopwords, lemas/tokens).  
- Vetorizar (BoW/TF-IDF/SBERT) e calcular **similaridade** (cosseno).  
- Clusterizar (K-Means) e projetar (PCA) para an√°lise explorat√≥ria.  
- Recomendar itens:  
  - por **nome** (itens similares);  
  - por **consulta** (busca sem√¢ntica);  
  - **substitutos** (mesma *Voc* e toler√¢ncia de *Lvl*).

---

## üóÇÔ∏è Estrutura do Projeto

```
pln-tibiawiki/
‚îÇ   analyze_vectorizer_items.py     # L√™ vetores e gera clusters/PCA/similaridade -> data/data_analyze_vectorizer
‚îÇ   get_tibiaitens_info.py          # Extrai atributos detalhados de cada link
‚îÇ   get_tibiaitens_links.py         # Coleta links de itens no TibiaWiki
‚îÇ   preprocess_csv.py               # Normaliza√ß√£o, stopwords, lemas/tokens
‚îÇ   recommendation_items.py         # Clustering, PCA, recomenda√ß√£o, substitutos
‚îÇ   vectorizer_items.py             # BoW/TF-IDF/SBERT + similaridade do cosseno
‚îÇ   requirements.txt
‚îÇ   README.md
‚îÇ
‚îî‚îÄ‚îÄ data/
    ‚îÇ   items_info.csv
    ‚îÇ   items_info_preprocessed.csv
    ‚îÇ
    ‚îú‚îÄ‚îÄ data_vectorizer/            # ENTRADA (vetores)
    ‚îÇ   ‚îú‚îÄ‚îÄ bow_matrix.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ tfidf_matrix.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ sbert_embeddings.npy
    ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
    ‚îÇ
    ‚îî‚îÄ‚îÄ data_analyze_vectorizer/    # SA√çDA (an√°lises)
        ‚îú‚îÄ‚îÄ cluster_labels.csv
        ‚îú‚îÄ‚îÄ cluster_pca2d.csv
        ‚îî‚îÄ‚îÄ similarity_sbert.csv
```

> **Aten√ß√£o:** os CSVs de matrizes usam `sep=";"`. Mantenha esse separador ao ler/salvar.

---

## ‚öôÔ∏è Ambiente & Instala√ß√£o

**Requisitos**
- Python **3.10+**  
- Internet na **primeira execu√ß√£o do SBERT** (download autom√°tico do modelo)

**Instala√ß√£o r√°pida**
```bash
# Criar e ativar ambiente
python -m venv .venv
# Windows:
.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate

# Instalar depend√™ncias
pip install --upgrade pip
pip install -r requirements.txt

# Baixar recursos NLTK usados no pr√©-processamento
python - << 'PY'
import nltk
nltk.download('punkt')
nltk.download('stopwords')
PY
```

**Requisitos do Selenium (opcional no scraping)**  
Caso use Selenium, recomendo `webdriver-manager` (j√° no requirements) para gerenciar o driver automaticamente.

---

## ‚ñ∂Ô∏è Como Executar (Passo a Passo)

> Voc√™ pode pular etapas se j√° tiver os artefatos gerados.

```bash
# (A) Coletar links de itens (gera data/items_links.csv)
python get_tibiaitens_links.py

# (B) Extrair informa√ß√µes de cada item (gera data/items_info.csv)
python get_tibiaitens_info.py

# (C) Pr√©-processar (gera data/items_info_preprocessed.csv)
python preprocess_csv.py

# (D) Vetorizar (gera BoW/TF-IDF/SBERT em data/data_vectorizer/)
python vectorizer_items.py

# (E) Analisar (clusters, PCA, similaridade ‚Üí data/data_analyze_vectorizer/)
python analyze_vectorizer_items.py
```

### Exemplos de Uso (recomenda√ß√µes)
No `analyze_vectorizer_items.py`, ajuste as chamadas de exemplo:
- **Por nome:** `recommend_items_by_name("Alicorn Quiver", df, vectors, top_k=5)`  
- **Por consulta:** `search_items("arco m√°gico para paladin", df, vectors, top_k=10)`  
- **Substitutos (mesma Voc + |ŒîLvl| <= toler√¢ncia):** `recommend_substitute_items("Alicorn Quiver", df, vectors, top_k=5)`

---

## üì¶ Artefatos Gerados

- **Vetores (entrada da an√°lise)** ‚Üí `data/data_vectorizer/`  
  `bow_matrix.csv`, `tfidf_matrix.csv`, `sbert_embeddings.npy`, `metadata.json`

- **An√°lises** ‚Üí `data/data_analyze_vectorizer/`  
  `cluster_labels.csv` (r√≥tulos K-Means), `cluster_pca2d.csv` (PCA 2D), `similarity_sbert.csv` (matriz de similaridade)

---

## üõ†Ô∏è Troubleshooting

- **SBERT n√£o roda / baixa modelo** ‚Üí precisa de internet na 1¬™ execu√ß√£o.  
- **Erro no KMeans (`n_init="auto"`)** ‚Üí troque para `n_init=10` ou garanta `scikit-learn >= 1.4`.  
- **CSV ‚Äútudo em uma coluna‚Äù** ‚Üí faltou `sep=";"` no `read_csv`.  
- **Selenium/driver** ‚Üí use `webdriver-manager` ou configure o ChromeDriver manualmente.

---

## üìö Refer√™ncias

- Sentence-Transformers (SBERT) ‚Äî buscas sem√¢nticas multil√≠ngues  
- scikit-learn ‚Äî K-Means, PCA, vetorizadores (BoW/TF-IDF)  
- NLTK ‚Äî tokeniza√ß√£o, stopwords  
- BeautifulSoup / Selenium ‚Äî scraping

---

## üìã Requirements (para `requirements.txt`)

> Use estas vers√µes m√≠nimas (ou fixe com `==` se quiser reprodutibilidade forte).

```
# Core
numpy>=1.26
scipy>=1.11
pandas>=2.0
scikit-learn>=1.4

# NLP
nltk>=3.9
sentence-transformers>=2.6

# Scraping
requests>=2.32
beautifulsoup4>=4.12
selenium>=4.22
webdriver-manager>=4.0

# Visualiza√ß√£o (se usar gr√°ficos/wordcloud em EDA/relat√≥rio)
matplotlib>=3.8
wordcloud>=1.9

# Utilit√°rios
tqdm>=4.66
```

**Instala√ß√£o direta via pip (alternativa):**
```bash
pip install numpy scipy pandas scikit-learn nltk sentence-transformers             requests beautifulsoup4 selenium webdriver-manager             matplotlib wordcloud tqdm
```

**Recursos NLTK (rodar uma vez):**
```bash
python - << 'PY'
import nltk
nltk.download('punkt')
nltk.download('stopwords')
PY
```
